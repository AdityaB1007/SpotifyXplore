{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a98648e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "DATA_FILE = r\"C:\\Users\\adity\\Downloads\\spotify_1mill_dataset\\spotify_data.csv\" \n",
    "NUM_USERS = 100             \n",
    "EMBEDDING_DIM = 32\n",
    "BATCH_SIZE = 256              \n",
    "EPOCHS = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_process_data(filepath):\n",
    "    \"\"\"\n",
    "    Loads the real Spotify dataset and prepares it for the model.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Could not find {filepath}. Please upload your dataset.\")\n",
    "\n",
    "    print(f\"Loading data from {filepath}...\")\n",
    "    \n",
    "    try:\n",
    "        tracks_df = pd.read_csv(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    required_cols = [\n",
    "        'track_id', 'artist_name', 'track_name', 'popularity', 'year', 'genre',\n",
    "        'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n",
    "        'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n",
    "        'duration_ms', 'time_signature'\n",
    "    ]\n",
    "    \n",
    "    existing_cols = [c for c in required_cols if c in tracks_df.columns]\n",
    "    tracks_df = tracks_df[existing_cols].copy()\n",
    "    \n",
    "    tracks_df.dropna(inplace=True)\n",
    "    \n",
    "    tracks_df.drop_duplicates(subset=['track_id'], inplace=True)\n",
    "\n",
    "    print(f\"Data Loaded: {len(tracks_df)} unique tracks.\")\n",
    "\n",
    "    print(\"Scaling audio features...\")\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    scale_cols = ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', \n",
    "                  'instrumentalness', 'liveness', 'valence', 'tempo']\n",
    "    \n",
    "    for col in scale_cols:\n",
    "        if col in tracks_df.columns:\n",
    "            tracks_df[col] = pd.to_numeric(tracks_df[col], errors='coerce')\n",
    "    \n",
    "    tracks_df.dropna(subset=scale_cols, inplace=True)\n",
    "\n",
    "    scaled_data = scaler.fit_transform(tracks_df[scale_cols])\n",
    "    scaled_cols_names = [f'scaled_{col}' for col in scale_cols]\n",
    "    tracks_df[scaled_cols_names] = scaled_data\n",
    "\n",
    "    print(\"Clustering songs based on audio features...\")\n",
    "    kmeans = KMeans(n_clusters=8, random_state=42)\n",
    "    tracks_df['cluster_label'] = kmeans.fit_predict(tracks_df[scaled_cols_names])\n",
    "    \n",
    "    # Simulate User Interactions\n",
    "\n",
    "    print(\"Simulating user listening history for NCF training...\")\n",
    "    interactions = []\n",
    "    \n",
    "    all_track_ids = tracks_df['track_id'].values\n",
    "    \n",
    "    for user_id in range(NUM_USERS):\n",
    "        num_listened = np.random.randint(20, 50)\n",
    "        \n",
    "        preferred_cluster = np.random.choice(range(8))\n",
    "        \n",
    "        cluster_songs = tracks_df[tracks_df['cluster_label'] == preferred_cluster]['track_id'].values\n",
    "        other_songs = tracks_df[tracks_df['cluster_label'] != preferred_cluster]['track_id'].values\n",
    "        \n",
    "        listened_tracks = []\n",
    "        if len(cluster_songs) > 0:\n",
    "             count_pref = min(len(cluster_songs), int(num_listened * 0.7))\n",
    "             listened_tracks.extend(np.random.choice(cluster_songs, count_pref, replace=False))\n",
    "        \n",
    "        needed = num_listened - len(listened_tracks)\n",
    "        if needed > 0 and len(other_songs) > 0:\n",
    "            listened_tracks.extend(np.random.choice(other_songs, needed, replace=False))\n",
    "            \n",
    "        for track in listened_tracks:\n",
    "            interactions.append({'user_id': user_id, 'track_id': track, 'interaction': 1.0})\n",
    "            \n",
    "    interaction_df = pd.DataFrame(interactions)\n",
    "    \n",
    "    return tracks_df, interaction_df, scaled_cols_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783ef36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NCFModel(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=32):\n",
    "        super(NCFModel, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embed = self.user_embedding(user_indices)\n",
    "        item_embed = self.item_embedding(item_indices)\n",
    "        x = torch.cat([user_embed, item_embed], dim=1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "class SpotifyDataset(Dataset):\n",
    "    def __init__(self, user_ids, item_ids, ratings):\n",
    "        self.user_ids = torch.tensor(user_ids, dtype=torch.long)\n",
    "        self.item_ids = torch.tensor(item_ids, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(ratings, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.item_ids[idx], self.ratings[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8676c541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from C:\\Users\\adity\\Downloads\\spotify_1mill_dataset\\spotify_data.csv...\n",
      "Data Loaded: 1159748 unique tracks.\n",
      "Scaling audio features...\n",
      "Clustering songs based on audio features...\n",
      "Simulating user listening history for NCF training...\n",
      "Encoding User and Item IDs...\n",
      "Initializing Model for 100 users and 1159748 items...\n",
      "Starting training on cuda...\n",
      "Epoch 1/10 - Loss: 0.7059\n",
      "Epoch 2/10 - Loss: 0.5164\n",
      "Epoch 3/10 - Loss: 0.3001\n",
      "Epoch 4/10 - Loss: 0.1276\n",
      "Epoch 5/10 - Loss: 0.0477\n",
      "Epoch 6/10 - Loss: 0.0212\n",
      "Epoch 7/10 - Loss: 0.0114\n",
      "Epoch 8/10 - Loss: 0.0075\n",
      "Epoch 9/10 - Loss: 0.0053\n",
      "Epoch 10/10 - Loss: 0.0041\n",
      "Saving model and data...\n",
      "Training complete. Files saved: ncf_model.pth, processed_tracks.pkl, encoders.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_pipeline():\n",
    "    \n",
    "    tracks_df, interactions_df, scaled_cols = load_and_process_data(DATA_FILE)\n",
    "    \n",
    "    if tracks_df is None:\n",
    "        return \n",
    "    \n",
    "    print(\"Encoding User and Item IDs...\")\n",
    "    user_encoder = LabelEncoder()\n",
    "    item_encoder = LabelEncoder()\n",
    "    \n",
    "    interactions_df['user_idx'] = user_encoder.fit_transform(interactions_df['user_id'])\n",
    "    \n",
    "    item_encoder.fit(tracks_df['track_id'])\n",
    "    interactions_df['item_idx'] = item_encoder.transform(interactions_df['track_id'])\n",
    "\n",
    "    train_data = SpotifyDataset(\n",
    "        interactions_df['user_idx'].values,\n",
    "        interactions_df['item_idx'].values,\n",
    "        interactions_df['interaction'].values\n",
    "    )\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    num_users = interactions_df['user_idx'].nunique()\n",
    "    num_items = len(item_encoder.classes_)\n",
    "    \n",
    "    print(f\"Initializing Model for {num_users} users and {num_items} items...\")\n",
    "    model = NCFModel(num_users, num_items, EMBEDDING_DIM).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    print(f\"Starting training on {DEVICE}...\")\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        for user_batch, item_batch, rating_batch in train_loader:\n",
    "            user_batch, item_batch, rating_batch = user_batch.to(DEVICE), item_batch.to(DEVICE), rating_batch.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(user_batch, item_batch).squeeze()\n",
    "            loss = criterion(predictions, rating_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    print(\"Saving model and data...\")\n",
    "    torch.save(model.state_dict(), \"ncf_model.pth\")\n",
    "    tracks_df.to_pickle(\"processed_tracks.pkl\")\n",
    "    \n",
    "    with open(\"encoders.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"user_encoder\": user_encoder, \n",
    "            \"item_encoder\": item_encoder,\n",
    "            \"scaled_cols\": scaled_cols\n",
    "        }, f)\n",
    "\n",
    "    print(\"Training complete. Files saved: ncf_model.pth, processed_tracks.pkl, encoders.pkl\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f097be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de15b228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a7d91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
